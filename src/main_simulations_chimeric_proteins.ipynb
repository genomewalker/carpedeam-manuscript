{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d167c6c-06a6-4dd2-9c83-85867017f418",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Bio'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mglob\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mBio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SeqIO\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Bio'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "from Bio import SeqIO\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import hashlib\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dee711b-9592-4114-9881-ec403532eaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_md5sum(x):\n",
    "    return hashlib.md5(x.encode(\"utf-8\")).hexdigest()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4458945-1682-44b7-9205-0c2cdcff51da",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = ['carpedeam2.configSafe', 'carpedeam2.configUnsafe', 'megahit.config0', 'penguin.config0', 'spades.config0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77b6b38-60f9-4196-b0f0-ebc68f540f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets=[\"ancientCalc\", \"ancientGut\", \"ancientHorse\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabd2524-c120-4976-84d3-d7b0d8c4c084",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"gut_sum_high_c3\", \"gut_sum_high_c5\", \"gut_sum_high_c10\", \"gut_sum_mid_c3\", \"gut_sum_mid_c5\", \"gut_sum_mid_c10\", \\\n",
    " \"calc_2095_high_c3\", \"calc_2095_high_c5\", \"calc_2095_high_c10\", \"calc_2095_mid_c3\", \"calc_2095_mid_c5\", \"calc_2095_mid_c10\", \\\n",
    " \"horse_sum_high_c3\", \"horse_sum_high_c5\", \"horse_sum_high_c10\", \"horse_sum_mid_c3\", \"horse_sum_mid_c5\", \"horse_sum_mid_c10\"]\n",
    "\n",
    "labels_clean = [\n",
    "    \"Gut:\\nHigh Damage; Cov. 3X\",\n",
    "    \"Gut:\\nHigh Damage; Cov. 5X\",\n",
    "    \"Gut:\\nHigh Damage; Cov. 10X\",\n",
    "    \"Gut:\\nMid Damage; Cov. 3X\",\n",
    "    \"Gut:\\nMid Damage; Cov. 5X\",\n",
    "    \"Gut:\\nMid Damage; Cov. 10X\",\n",
    "    \"Calculus:\\nHigh Damage; Cov. 3X\",\n",
    "    \"Calculus:\\nHigh Damage; Cov. 5X\",\n",
    "    \"Calculus:\\nHigh Damage; Cov. 10X\",\n",
    "    \"Calculus:\\nMid Damage; Cov. 3X\",\n",
    "    \"Calculus:\\nMid Damage; Cov. 5X\",\n",
    "    \"Calculus:\\nMid Damage; Cov. 10X\",\n",
    "    \"Bone:\\nHigh Damage; Cov. 3X\",\n",
    "    \"Bone:\\nHigh Damage; Cov. 5X\",\n",
    "    \"Bone:\\nHigh Damage; Cov. 10X\",\n",
    "    \"Bone:\\nMid Damage; Cov. 3X\",\n",
    "    \"Bone:\\nMid Damage; Cov. 5X\",\n",
    "    \"Bone:\\nMid Damage; Cov. 10X\"\n",
    "]\n",
    "\n",
    "labels_dict = {key: get_md5sum(key) for key in labels}\n",
    "labels_dict_inv = {value: key for key, value in labels_dict.items()}\n",
    "print(labels_dict_inv)\n",
    "\n",
    "labels_dict_clean = {labels[i] : labels_clean[i] for i in range(len(labels))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d515febf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_assembler(cell):\n",
    "    if \"carpedeam\" in cell:\n",
    "        return \"CarpeDeam\"\n",
    "    elif \"penguin\" in cell:\n",
    "        return \"PenguiN\"\n",
    "    elif \"megahit\" in cell:\n",
    "        return \"MEGAHIT\"\n",
    "    elif \"spades\" in cell:\n",
    "        return \"metaSPAdes\"\n",
    "    else:\n",
    "        return cell  # Return the cell as is if none of the conditions are met\n",
    "\n",
    "def adjust_assemblerconfig(row):\n",
    "    if row[\"assembler\"] == \"CarpeDeam\":\n",
    "        if \"Safe\" in row[\"file\"]:\n",
    "            return \"CarpeDeam (safe mode)\"\n",
    "        elif \"Unsafe\" in row[\"file\"]:\n",
    "            return \"CarpeDeam (unsafe mode)\"\n",
    "        else:\n",
    "            return \"CarpeDeam\\n(safe mode)\"\n",
    "    else:\n",
    "        return row[\"assembler\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39026e1-c5c7-482e-ab42-2775bcbf6a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def curate_report_chimera_df(file):\n",
    "    \"\"\"\n",
    "    Returns a list of dataframes. Each dataframe belongs to a file/assembler. The analyzed files are from mmseq taxonomy:\n",
    "    (1) Query\n",
    "    (2) Target\n",
    "    (3) Seq.Id.\n",
    "    (4) Alignment Length\n",
    "    (5) Number of mismatches\n",
    "    (6) number of gap openings\n",
    "    (7) Start in Query\n",
    "    (8) End in Query\n",
    "    (9) Start in Target\n",
    "    (10) End in Target\n",
    "    (11) Eval\n",
    "    (12) bit score\n",
    "    \"\"\"\n",
    "\n",
    "    # Read the file into a pandas DataFrame\n",
    "    df_aln = pd.read_csv(file, sep='\\t', names=[\"query\", \"target\", \"seq.Id.\", \"alnLen\", \"MM\", \"gaps\", \"startQuery\", \"EndQuery\", \"startTarget\", \\\n",
    "                                                \"EndTarget\", \"Eval\", \"bit score\", \"queryLen\", \"targetLen\", \"queryCov\", \"targetCov\"])\n",
    "\n",
    "    df_aln = df_aln[ df_aln[\"seq.Id.\"] >= 0.9 ]\n",
    "    df_aln = df_aln[ df_aln[\"alnLen\"] >= 100 ]    \n",
    "    \n",
    "    return df_aln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7cd2fb-1f5f-4004-a7b9-4c5ab59f3506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_uncovered_intervals(df, min_gap):\n",
    "    uncovered_queries = set()\n",
    "    total_uncovered_length = 0  # To accumulate the total length of all uncovered intervals\n",
    "    total_target_lengths = 0\n",
    "\n",
    "    for target, group in df.groupby('target'):\n",
    "        # Sort intervals based on start positions\n",
    "        intervals = sorted(zip(group['startTarget'], group['EndTarget'], group['query']))\n",
    "        # Calculate uncovered gaps\n",
    "        uncovered_length = 0  # Length of uncovered intervals for the current target\n",
    "\n",
    "        # Assuming we have a target length column (for end gap check)\n",
    "        target_length = group['targetLen'].iloc[0]\n",
    "        total_target_lengths += target_length\n",
    "\n",
    "        # First gap (before the first interval)\n",
    "        if intervals[0][0] > min_gap:\n",
    "            uncovered_length += intervals[0][0]  # Add the gap length before the first interval\n",
    "            uncovered_queries.add(intervals[0][2])  # Add the query ID of the first interval\n",
    "\n",
    "        # Gaps between intervals\n",
    "        for i in range(1, len(intervals)):\n",
    "            gap = intervals[i][0] - intervals[i-1][1] - 1\n",
    "            if gap > min_gap:\n",
    "                uncovered_length += gap\n",
    "                uncovered_queries.add(intervals[i-1][2])  # Add the previous query ID\n",
    "                uncovered_queries.add(intervals[i][2])   # Add the current query ID\n",
    "\n",
    "        # Final gap (after the last interval)\n",
    "        if target_length - intervals[-1][1] > min_gap:\n",
    "            uncovered_length += target_length - intervals[-1][1]\n",
    "            uncovered_queries.add(intervals[-1][2])  # Add the query ID of the last interval\n",
    "\n",
    "        # Update total uncovered length\n",
    "        total_uncovered_length += uncovered_length\n",
    "\n",
    "    if total_target_lengths != 0:\n",
    "        ratio = total_uncovered_length / total_target_lengths\n",
    "    else:\n",
    "        ratio = 0  # or you might set it to some default or indicative value\n",
    "\n",
    "    return ratio, list(uncovered_queries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808b7a11-429f-4869-be27-f662f89ac076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_protein_matches(directory, configs, damage, minGap):\n",
    "    # Find all FASTA files in the specified directory\n",
    "    tsv_files = glob.glob(os.path.join(directory, \"*.tsv\"))\n",
    "    \n",
    "    # Dictionary to store the information\n",
    "    protein_info = {}\n",
    "\n",
    "    for file in tsv_files:\n",
    "\n",
    "        if not any(config in file for config in configs):\n",
    "            continue\n",
    "\n",
    "        name = os.path.basename(file)\n",
    "        assembler = re.search(r'mmseqs.([a-zA-Z0-9]+).config', name).group(1)\n",
    "        label = re.match(r'([a-z0-9]+).raw', name).group(1)\n",
    "        label_human = labels_dict_inv.get(label, \"ERROR\")  # Replace default_value with the desired default if label is not found\n",
    "        dam = label_human.split(\"_\")[2]\n",
    "        if dam != damage:\n",
    "            continue\n",
    "        \n",
    "        config = re.search(r'config(\\d+)', name).group(1)\n",
    "        \n",
    "        chimeradf = curate_report_chimera_df(file)\n",
    "        \n",
    "        ratio_nohits, unique_hits_list = find_uncovered_intervals(chimeradf, minGap)\n",
    "        unique_hits = len(unique_hits_list)\n",
    "        \n",
    "        protein_info[name] = [assembler, label, config, unique_hits, ratio_nohits]\n",
    "        \n",
    "\n",
    "    print(\"iteration done\")\n",
    "    # Create a DataFrame from the dictionary\n",
    "    df = pd.DataFrame.from_dict(protein_info, orient='index', columns=['Assembler', 'Label', 'Config', 'UniqueHits', 'ratioNoHits'])\n",
    "    df.reset_index(inplace=True)\n",
    "    df.rename(columns={'index': 'Filename'}, inplace=True)\n",
    "\n",
    "    # Add additional columns\n",
    "    df[\"assemblerconfig\"] = df[\"Assembler\"] + \" \" + df[\"Config\"]\n",
    "    df[\"assembler_clean\"] = df[\"Assembler\"].apply(map_assembler)\n",
    "    df[\"assembler_final\"] = df.apply(adjust_assemblerconfig, axis=1)\n",
    "    df[\"label\"] = df[\"Label\"].astype(str)\n",
    "    df[\"label_human\"] = df[\"Label\"].map(labels_dict_inv)\n",
    "    df[\"label_clean\"] = df[\"label_human\"].replace(labels_dict_clean)\n",
    "    df[\"dataset_clean\"] = df[\"label_clean\"].str.split(\":\").str[0]\n",
    "    df[\"coverage\"] = df[\"label_human\"].str.split(\"_\").str[3].str[1::]\n",
    "    df[\"damage\"] = df[\"label_human\"].str.split(\"_\").str[2]\n",
    "\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae87ad6e-82e7-4141-be6b-9f3f4d68d239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def curate_df(datasets, configs, damage, minGap):\n",
    "\n",
    "    dfs = []\n",
    "    for data in datasets:\n",
    "        directory = f\"data/{data}/results/assembly-mmseqs/prokka_vs_prokka_chimera\"\n",
    "        df = filter_protein_matches(directory, configs, damage, minGap)\n",
    "        dfs.append(df)\n",
    "        \n",
    "    big_df = pd.concat(dfs, ignore_index=True)\n",
    "    return big_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d38a41-2f46-45b8-841b-9b83e6bdd5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ratio_noHits(df_orig, damage, minGap, bar_width=1):\n",
    "    # Filter dataframe based on damage\n",
    "    df = df_orig[df_orig[\"damage\"] == damage]\n",
    "    \n",
    "    # Define a custom palette for consistent color coding\n",
    "    custom_palette = ['#a1c9f4', '#b9f2f0', '#8de5a1', '#ffb482', '#fab0e4']\n",
    "    \n",
    "    # Set custom order for assemblers\n",
    "    custom_order = [\n",
    "        'CarpeDeam\\n(safe mode)',\n",
    "        'CarpeDeam\\n(unsafe mode)',\n",
    "        'PenguiN',\n",
    "        'MEGAHIT',\n",
    "        'metaSPAdes'\n",
    "    ]\n",
    "    \n",
    "    assembler_order = sorted(df[\"assembler_final\"].unique(), key=lambda x: custom_order.index(x))\n",
    "    \n",
    "    # Convert 'assembler_final' to categorical type with custom order\n",
    "    df['assembler_final'] = pd.Categorical(df['assembler_final'], categories=assembler_order, ordered=True)\n",
    "    \n",
    "    # Set the aesthetic style of the plots\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    \n",
    "    # Set custom order for coverages\n",
    "    coverage_order = [\"3\", \"5\", \"10\"]\n",
    "    df['coverage'] = pd.Categorical(df['coverage'], categories=coverage_order, ordered=True)\n",
    "    \n",
    "    # Determine the number of unique values for dataset_clean\n",
    "    datasets = sorted(df['dataset_clean'].unique())\n",
    "    \n",
    "    # Set up the subplots grid with appropriate size\n",
    "    fig_width = 12\n",
    "    fig_height = 9\n",
    "    fig, axs = plt.subplots(3, 3, figsize=(fig_width, fig_height), sharey=False)\n",
    "    \n",
    "    # Plot each subplot\n",
    "    plot_idx = 0\n",
    "    for dataset in datasets:\n",
    "        for coverage in coverage_order:\n",
    "            i, j = divmod(plot_idx, 3)  # Determine subplot position\n",
    "            if i < 3 and j < 3:\n",
    "                ax = axs[i, j]\n",
    "                subset = df[(df['dataset_clean'] == dataset) & (df['coverage'] == coverage) & (df['ratioNoHits'] != 0)]                \n",
    "                if not subset.empty:\n",
    "                    sns.barplot(\n",
    "                        x='ratioNoHits', \n",
    "                        y='assembler_final', \n",
    "                        hue='assembler_final', \n",
    "                        data=subset, \n",
    "                        ax=ax, \n",
    "                        palette=custom_palette, \n",
    "                        hue_order=assembler_order,\n",
    "                        errorbar=None,\n",
    "                        dodge=False,\n",
    "                        orient='h',  # Set orientation to horizontal\n",
    "                        width=bar_width\n",
    "                    )\n",
    "                    ax.set_title(f'Dataset: {dataset}, Coverage: {coverage}X')\n",
    "                    ax.set_ylabel('Assembler')\n",
    "                    if j == 1:\n",
    "                        ax.set_xlabel(f'Fraction of Amino Acids (Belonging to a Window of at Least {minGap} AA\\'s) Not Matching Any Reference')\n",
    "                    else:\n",
    "                        ax.set_xlabel('')\n",
    "                    ax.tick_params(axis='y', rotation=0, labelsize=10)\n",
    "                    #ax.legend().set_visible(False)\n",
    "                plot_idx += 1\n",
    "    \n",
    "    # Remove empty subplots\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            if i * 3 + j >= plot_idx:\n",
    "                fig.delaxes(axs[i][j])\n",
    "    \n",
    "    # Add a single legend below the plot if there are handles\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    if handles:\n",
    "        fig.legend(handles, labels, title='Assembler', bbox_to_anchor=(0.5, -0.05), loc='upper center', ncol=len(labels))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/proteins_horizontal_ratio_nohits_{minGap}_min90_repro.svg', format=\"svg\", bbox_inches=\"tight\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff5aad2-07c8-4469-ba16-b741a0f8d2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "minGapList = [20,60,100,200]\n",
    "minGapList = [100]\n",
    "minGapDFs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa327af-df15-43e7-ab30-7cd72cf885ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for minGap in minGapList:\n",
    "    protein_results = curate_df(datasets, configs, \"high\", minGap)\n",
    "    minGapDFs.append(protein_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1793af78-dbc4-4496-a819-0e653485fc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "for minGapDF, minGap in zip(minGapDFs, minGapList):\n",
    "    #plot_num_chimera_horizontal(minGapDF, \"high\", minGap)\n",
    "    plot_ratio_noHits(minGapDF, \"high\", minGap)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
